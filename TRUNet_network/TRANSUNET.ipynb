{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "from statistics import fmean\n",
        "\n",
        "\n",
        "def dice_score(pred, gt):  # data in shape [batch, classes, h, w, d]\n",
        "    dice = []\n",
        "    for batchloop in range(gt.shape[0]):\n",
        "        dice_tmp = []\n",
        "        for roi in range(gt.shape[1]):\n",
        "            if roi > 0:  # skip background\n",
        "                pred_tmp = pred[int(batchloop), int(roi)]\n",
        "                gt_tmp = gt[int(batchloop), int(roi)]\n",
        "                a = np.sum(pred_tmp[gt_tmp == 1])\n",
        "                b = np.sum(pred_tmp)\n",
        "                c = np.sum(gt_tmp)\n",
        "                if a == 0:\n",
        "                    metric = 0\n",
        "                else:\n",
        "                    metric_ = a * 2.0 / (b + c)\n",
        "                    metric = metric_.item()\n",
        "                dice_tmp.append(metric)\n",
        "        dice.append(fmean(dice_tmp))\n",
        "    return fmean(dice)\n",
        "\n",
        "\n",
        "def one_hot_encoder(input_tensor, n_classes):\n",
        "    tensor_list = []\n",
        "    for i in range(n_classes):\n",
        "        temp_prob = input_tensor == i  # * torch.ones_like(input_tensor)\n",
        "        tensor_list.append(temp_prob)\n",
        "    output_tensor = torch.cat(tensor_list, dim=1)\n",
        "    return output_tensor.float()\n",
        "\n",
        "\n",
        "def to_one_arr_encoding(input_tensor):  # input shape: [batch, channels, h, w, d]\n",
        "    new_arr = torch.zeros(input_tensor.shape)\n",
        "    for batchloop in range(input_tensor.shape[0]):\n",
        "        for d in range(input_tensor.shape[1]):\n",
        "            new_arr[batchloop, d] = torch.where(input_tensor[batchloop, d] == 1, d + 1, 0)\n",
        "    return new_arr.sum(1).unsqueeze(1)\n",
        "\n",
        "\n",
        "def trainer(args, config, model, savepath):\n",
        "    # Initializations\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda:0\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Parameters\n",
        "    loss_function = config['loss_function']\n",
        "    optimizer = config['optimizer']\n",
        "    dataset_train = config['ds_train']\n",
        "    dataset_val = config['ds_val']\n",
        "    save_interval = config['save_interval']\n",
        "\n",
        "    def worker_init_fn(worker_id):\n",
        "        random.seed(args.seed + worker_id)\n",
        "\n",
        "    # Data Loaders\n",
        "    train_loader = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=16, pin_memory=True,\n",
        "                              worker_init_fn=worker_init_fn)\n",
        "    val_loader = DataLoader(dataset_val, batch_size=1, shuffle=True, num_workers=2, pin_memory=True,\n",
        "                            worker_init_fn=worker_init_fn)\n",
        "\n",
        "    max_iterations = args.max_epochs * len(train_loader)\n",
        "\n",
        "    # logging\n",
        "    logging.basicConfig(filename=args.save_path + \"/log.txt\", level=logging.INFO,\n",
        "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
        "    logging.info(args)\n",
        "    logging.info(\"{} iterations per epoch. {} max iterations \".format(len(train_loader), max_iterations))\n",
        "    writer = SummaryWriter(savepath + '/log')\n",
        "\n",
        "    best_metric = -1\n",
        "    best_metric_epoch = -1\n",
        "    metric_values = []\n",
        "    iter_num = 0\n",
        "\n",
        "    ############################\n",
        "    #         Training         #\n",
        "    ############################\n",
        "\n",
        "    for epoch in range(args.max_epochs):\n",
        "        epoch_loss = 0\n",
        "        model.train()\n",
        "        for i_batch, sampled_batch in enumerate(train_loader):\n",
        "            # get inputs and targets\n",
        "            inputs, targets = sampled_batch['image'], sampled_batch['label']\n",
        "            # here the input and target have the shape [batch, H, L, D]\n",
        "            # so we need to add the channel dimension\n",
        "            inputs, targets = inputs.unsqueeze(1), targets.unsqueeze(1)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update learning rate\n",
        "            epoch_loss += loss\n",
        "            lr_ = args.base_lr * (1.0 - iter_num / max_iterations) ** 0.9\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_\n",
        "            lrlog = lr_\n",
        "            writer.add_scalar('info/lr', lr_, iter_num)\n",
        "            iter_num = iter_num + 1\n",
        "\n",
        "            # write to log\n",
        "            writer.add_scalar('info/total_loss', loss, iter_num)\n",
        "            # logging.info('iteration %d : loss : %f' % (iter_num, loss.item()))\n",
        "\n",
        "        epoch_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "        logging.info('epoch %d : mean loss : %f' % (epoch, epoch_loss))\n",
        "\n",
        "        ############################\n",
        "        #        Validation        #\n",
        "        ############################\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            dice_tmp = []\n",
        "            for i_batch, sampled_batch in enumerate(val_loader):\n",
        "                # get inputs and targets\n",
        "                inputs, targets = sampled_batch['image'], sampled_batch['label']\n",
        "                # targets needs to be transformed to one-hot encoding\n",
        "                targets = targets.unsqueeze(1)\n",
        "                targets = one_hot_encoder(targets, args.num_classes)\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                val_outputs = model(inputs)\n",
        "                m = nn.Softmax(dim=1)\n",
        "                val_outputs = m(val_outputs)\n",
        "\n",
        "                # compute metric for current iteration\n",
        "                dice_tmp.append(dice_score(val_outputs.cpu().data.numpy(), targets.cpu().data.numpy()))\n",
        "\n",
        "            # aggregate the final mean dice result\n",
        "            metric = fmean(dice_tmp)\n",
        "\n",
        "            # write to log\n",
        "            writer.add_scalar('info/validation_metric', metric, epoch)\n",
        "            logging.info('iteration %d : dice score : %f' % (epoch, metric))\n",
        "\n",
        "            metric_values.append(metric)\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), os.path.join(args.save_path, \"best_metric_model.pth\"))\n",
        "                print(\"saved new best metric model\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}. Current learning rate {lrlog}\"\n",
        "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
        "                f\"at epoch: {best_metric_epoch}\"\n",
        "            )\n",
        "\n",
        "        ############################\n",
        "        #          Saving          #\n",
        "        ############################\n",
        "\n",
        "        # add an example to tensorboard logging\n",
        "        labs = to_one_arr_encoding(targets)\n",
        "        outputs = torch.argmax(torch.softmax(val_outputs, dim=1), dim=1, keepdim=True)\n",
        "\n",
        "        if len(inputs.shape) == 5:\n",
        "            image = inputs[:, :, :, :, round(args.img_size / 2)]\n",
        "            labs = labs[:, :, :, :, round(args.img_size / 2)]\n",
        "            outputs = outputs[:, :, :, round(args.img_size / 2)]\n",
        "        else:\n",
        "            image = inputs\n",
        "\n",
        "        image = image[0]\n",
        "        labs = torch.squeeze(labs * 50, 1)\n",
        "        outputs = outputs[0] * 50\n",
        "        image = (image - image.min()) / (image.max() - image.min())\n",
        "        writer.add_image('train/Image', image, iter_num)\n",
        "        writer.add_image('train/Prediction', outputs, iter_num)\n",
        "        writer.add_image('train/GroundTruth', labs, iter_num)\n",
        "\n",
        "        if (epoch + 1) % save_interval == 0:\n",
        "            save_mode_path = os.path.join(args.save_path, 'epoch_' + str(epoch) + '.pth')\n",
        "            torch.save(model.state_dict(), save_mode_path)\n",
        "            logging.info(\"save model to {}\".format(save_mode_path))\n",
        "\n",
        "        if epoch >= args.max_epochs - 1:\n",
        "            save_mode_path = os.path.join(args.save_path, 'epoch_' + str(epoch) + '.pth')\n",
        "            torch.save(model.state_dict(), save_mode_path)\n",
        "            logging.info(\"save model to {}\".format(save_mode_path))\n",
        "            break\n",
        "\n",
        "    writer.close()\n",
        "    return \"Training Finished!\""
      ],
      "metadata": {
        "id": "p60S4qp5GG3B",
        "outputId": "93eb34b1-1581-431f-f3ba-372868dd1996",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorboardX'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5c40fa5acdbe>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatistics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfmean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorboardX'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bU1pVxdAHGs_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}