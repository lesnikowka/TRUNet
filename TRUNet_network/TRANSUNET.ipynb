{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX\n",
        "!pip install ml_collections\n",
        "!pip install monai"
      ],
      "metadata": {
        "id": "p60S4qp5GG3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3328d414-a67c-49ea-b6de-dec64dd44e5e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (4.25.5)\n",
            "Requirement already satisfied: ml_collections in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.17.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml_collections) (6.0.2)\n",
            "Requirement already satisfied: monai in /usr/local/lib/python3.10/dist-packages (1.4.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.24 in /usr/local/lib/python3.10/dist-packages (from monai) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.9 in /usr/local/lib/python3.10/dist-packages (from monai) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9->monai) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9->monai) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9->monai) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch training https://github.com/lesnikowka/TRUNet.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dabum2IYKdZC",
        "outputId": "2156f5da-2438-4b4f-890d-d21507d520d0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TRUNet' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "from statistics import fmean\n",
        "\n",
        "\n",
        "def dice_score(pred, gt):  # data in shape [batch, classes, h, w, d]\n",
        "    dice = []\n",
        "    for batchloop in range(gt.shape[0]):\n",
        "        dice_tmp = []\n",
        "        for roi in range(gt.shape[1]):\n",
        "            if roi > 0:  # skip background\n",
        "                pred_tmp = pred[int(batchloop), int(roi)]\n",
        "                gt_tmp = gt[int(batchloop), int(roi)]\n",
        "                a = np.sum(pred_tmp[gt_tmp == 1])\n",
        "                b = np.sum(pred_tmp)\n",
        "                c = np.sum(gt_tmp)\n",
        "                if a == 0:\n",
        "                    metric = 0\n",
        "                else:\n",
        "                    metric_ = a * 2.0 / (b + c)\n",
        "                    metric = metric_.item()\n",
        "                dice_tmp.append(metric)\n",
        "        dice.append(fmean(dice_tmp))\n",
        "    return fmean(dice)\n",
        "\n",
        "\n",
        "def one_hot_encoder(input_tensor, n_classes):\n",
        "    tensor_list = []\n",
        "    for i in range(n_classes):\n",
        "        temp_prob = input_tensor == i  # * torch.ones_like(input_tensor)\n",
        "        tensor_list.append(temp_prob)\n",
        "    output_tensor = torch.cat(tensor_list, dim=1)\n",
        "    return output_tensor.float()\n",
        "\n",
        "\n",
        "def to_one_arr_encoding(input_tensor):  # input shape: [batch, channels, h, w, d]\n",
        "    new_arr = torch.zeros(input_tensor.shape)\n",
        "    for batchloop in range(input_tensor.shape[0]):\n",
        "        for d in range(input_tensor.shape[1]):\n",
        "            new_arr[batchloop, d] = torch.where(input_tensor[batchloop, d] == 1, d + 1, 0)\n",
        "    return new_arr.sum(1).unsqueeze(1)\n",
        "\n",
        "\n",
        "def trainer(args, config, model, savepath):\n",
        "    # Initializations\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda:0\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Parameters\n",
        "    loss_function = config['loss_function']\n",
        "    optimizer = config['optimizer']\n",
        "    dataset_train = config['ds_train']\n",
        "    dataset_val = config['ds_val']\n",
        "    save_interval = config['save_interval']\n",
        "\n",
        "    def worker_init_fn(worker_id):\n",
        "        random.seed(args.seed + worker_id)\n",
        "\n",
        "    # Data Loaders\n",
        "    train_loader = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True, num_workers=16, pin_memory=True,\n",
        "                              worker_init_fn=worker_init_fn)\n",
        "    val_loader = DataLoader(dataset_val, batch_size=1, shuffle=True, num_workers=2, pin_memory=True,\n",
        "                            worker_init_fn=worker_init_fn)\n",
        "\n",
        "    max_iterations = args.max_epochs * len(train_loader)\n",
        "\n",
        "    # logging\n",
        "    logging.basicConfig(filename=args.save_path + \"/log.txt\", level=logging.INFO,\n",
        "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
        "    logging.info(args)\n",
        "    logging.info(\"{} iterations per epoch. {} max iterations \".format(len(train_loader), max_iterations))\n",
        "    writer = SummaryWriter(savepath + '/log')\n",
        "\n",
        "    best_metric = -1\n",
        "    best_metric_epoch = -1\n",
        "    metric_values = []\n",
        "    iter_num = 0\n",
        "\n",
        "    ############################\n",
        "    #         Training         #\n",
        "    ############################\n",
        "\n",
        "    for epoch in range(args.max_epochs):\n",
        "        epoch_loss = 0\n",
        "        model.train()\n",
        "        for i_batch, sampled_batch in enumerate(train_loader):\n",
        "            # get inputs and targets\n",
        "            inputs, targets = sampled_batch['image'], sampled_batch['label']\n",
        "            # here the input and target have the shape [batch, H, L, D]\n",
        "            # so we need to add the channel dimension\n",
        "            inputs, targets = inputs.unsqueeze(1), targets.unsqueeze(1)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update learning rate\n",
        "            epoch_loss += loss\n",
        "            lr_ = args.base_lr * (1.0 - iter_num / max_iterations) ** 0.9\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = lr_\n",
        "            lrlog = lr_\n",
        "            writer.add_scalar('info/lr', lr_, iter_num)\n",
        "            iter_num = iter_num + 1\n",
        "\n",
        "            # write to log\n",
        "            writer.add_scalar('info/total_loss', loss, iter_num)\n",
        "            # logging.info('iteration %d : loss : %f' % (iter_num, loss.item()))\n",
        "\n",
        "        epoch_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "        logging.info('epoch %d : mean loss : %f' % (epoch, epoch_loss))\n",
        "\n",
        "        ############################\n",
        "        #        Validation        #\n",
        "        ############################\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            dice_tmp = []\n",
        "            for i_batch, sampled_batch in enumerate(val_loader):\n",
        "                # get inputs and targets\n",
        "                inputs, targets = sampled_batch['image'], sampled_batch['label']\n",
        "                # targets needs to be transformed to one-hot encoding\n",
        "                targets = targets.unsqueeze(1)\n",
        "                targets = one_hot_encoder(targets, args.num_classes)\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                val_outputs = model(inputs)\n",
        "                m = nn.Softmax(dim=1)\n",
        "                val_outputs = m(val_outputs)\n",
        "\n",
        "                # compute metric for current iteration\n",
        "                dice_tmp.append(dice_score(val_outputs.cpu().data.numpy(), targets.cpu().data.numpy()))\n",
        "\n",
        "            # aggregate the final mean dice result\n",
        "            metric = fmean(dice_tmp)\n",
        "\n",
        "            # write to log\n",
        "            writer.add_scalar('info/validation_metric', metric, epoch)\n",
        "            logging.info('iteration %d : dice score : %f' % (epoch, metric))\n",
        "\n",
        "            metric_values.append(metric)\n",
        "            if metric > best_metric:\n",
        "                best_metric = metric\n",
        "                best_metric_epoch = epoch + 1\n",
        "                torch.save(model.state_dict(), os.path.join(args.save_path, \"best_metric_model.pth\"))\n",
        "                print(\"saved new best metric model\")\n",
        "            print(\n",
        "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}. Current learning rate {lrlog}\"\n",
        "                f\"\\nbest mean dice: {best_metric:.4f} \"\n",
        "                f\"at epoch: {best_metric_epoch}\"\n",
        "            )\n",
        "\n",
        "        ############################\n",
        "        #          Saving          #\n",
        "        ############################\n",
        "\n",
        "        # add an example to tensorboard logging\n",
        "        labs = to_one_arr_encoding(targets)\n",
        "        outputs = torch.argmax(torch.softmax(val_outputs, dim=1), dim=1, keepdim=True)\n",
        "\n",
        "        if len(inputs.shape) == 5:\n",
        "            image = inputs[:, :, :, :, round(args.img_size / 2)]\n",
        "            labs = labs[:, :, :, :, round(args.img_size / 2)]\n",
        "            outputs = outputs[:, :, :, round(args.img_size / 2)]\n",
        "        else:\n",
        "            image = inputs\n",
        "\n",
        "        image = image[0]\n",
        "        labs = torch.squeeze(labs * 50, 1)\n",
        "        outputs = outputs[0] * 50\n",
        "        image = (image - image.min()) / (image.max() - image.min())\n",
        "        writer.add_image('train/Image', image, iter_num)\n",
        "        writer.add_image('train/Prediction', outputs, iter_num)\n",
        "        writer.add_image('train/GroundTruth', labs, iter_num)\n",
        "\n",
        "        if (epoch + 1) % save_interval == 0:\n",
        "            save_mode_path = os.path.join(args.save_path, 'epoch_' + str(epoch) + '.pth')\n",
        "            torch.save(model.state_dict(), save_mode_path)\n",
        "            logging.info(\"save model to {}\".format(save_mode_path))\n",
        "\n",
        "        if epoch >= args.max_epochs - 1:\n",
        "            save_mode_path = os.path.join(args.save_path, 'epoch_' + str(epoch) + '.pth')\n",
        "            torch.save(model.state_dict(), save_mode_path)\n",
        "            logging.info(\"save model to {}\".format(save_mode_path))\n",
        "            break\n",
        "\n",
        "    writer.close()\n",
        "    return \"Training Finished!\""
      ],
      "metadata": {
        "id": "bU1pVxdAHGs_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Добавляем путь к корневой папке проекта\n",
        "sys.path.append('/content/TRUNet')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPkfZ_1iK6XW",
        "outputId": "a7a3f4a9-5ccf-48b2-b9f9-b835b2b22fb7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import ml_collections\n",
        "import os\n",
        "from glob import glob\n",
        "import torch\n",
        "import numpy as np\n",
        "from monai.losses import DiceCELoss\n",
        "from monai.metrics import DiceMetric\n",
        "from TRUNet.TRUNet_network.model.ViT import VisionTransformer3d as TransUNet3d\n",
        "from datetime import datetime\n",
        "from TRUNet_network.trunet_train import trainer\n",
        "from torchvision import transforms\n",
        "from TRUNet_network.augmentations import RandomGenerator3d_zoom, Reshape3d_zoom\n",
        "\n",
        "now = datetime.now()\n",
        "\n",
        "def TransUNet_configs(img_size):\n",
        "    configs_trunet = ml_collections.ConfigDict()\n",
        "\n",
        "    configs_trunet.resnet = ml_collections.ConfigDict()\n",
        "    configs_trunet.resnet.num_layers = (3, 4, 9)\n",
        "    configs_trunet.resnet.width_factor = 1\n",
        "    configs_trunet.transformer_mlp_dim = 3072\n",
        "    configs_trunet.transformer_num_heads = 12\n",
        "    configs_trunet.transformer_num_layers = 12\n",
        "    configs_trunet.transformer_attention_dropout_rate = 0.0\n",
        "    configs_trunet.transformer_dropout_rate = 0.1\n",
        "    configs_trunet.classifier = 'seg'\n",
        "    configs_trunet.decoder_channels = (256, 128, 64, 16)\n",
        "    configs_trunet.n_classes = 7\n",
        "    configs_trunet.n_skip = 3\n",
        "    configs_trunet.skip_channels = [512, 256, 64, 16]\n",
        "    configs_trunet.patches = ml_collections.ConfigDict()\n",
        "    configs_trunet.patches.grid = None\n",
        "\n",
        "    configs_trunet.hidden_size = 768\n",
        "    configs_trunet.patches.size = 16\n",
        "\n",
        "    configs_trunet.patch_size = configs_trunet.patches.size  # (results in 14 by 14 grid of patches for input size 224)\n",
        "\n",
        "    configs_trunet.patches.grid = (\n",
        "        int(img_size / configs_trunet.patches.size), int(img_size / configs_trunet.patches.size),\n",
        "        int(img_size / configs_trunet.patches.size))\n",
        "    configs_trunet.hybrid = True\n",
        "\n",
        "    return configs_trunet\n",
        "\n",
        "\n",
        "class fetch_dataset:\n",
        "    def __init__(self, base_dir, crop=None, transform=None):\n",
        "        self.transform = transform\n",
        "        self.data_dir = base_dir\n",
        "        sample_list = sorted(glob(os.path.join(base_dir, '*.npz')))\n",
        "        self.sample_list = sample_list\n",
        "        self.name = [i.split('/')[-1].split('.npz')[0] for i in sample_list]\n",
        "        self.pt = [int(i.split('_')[0][2:]) for i in self.name]\n",
        "        self.crop = crop\n",
        "        if crop is None or crop == 'None':\n",
        "            pass\n",
        "        else:\n",
        "            with open(crop) as file:\n",
        "                lines = [line.rstrip() for line in file]\n",
        "            self.crop_pt = [int(i.split(' ')[0]) for i in lines]\n",
        "            self.crop_xmin = [float(i.split(' ')[1]) for i in lines]\n",
        "            self.crop_xmax = [float(i.split(' ')[2]) for i in lines]\n",
        "            self.crop_ymin = [float(i.split(' ')[3]) for i in lines]\n",
        "            self.crop_ymax = [float(i.split(' ')[4]) for i in lines]\n",
        "            self.crop_zmin = [float(i.split(' ')[5]) for i in lines]\n",
        "            self.crop_zmax = [float(i.split(' ')[6]) for i in lines]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        data_path = self.sample_list[idx]\n",
        "        data = np.load(data_path)\n",
        "        image, label = data['arr_0'], data['arr_1']\n",
        "\n",
        "        if self.crop is None or self.crop == 'None':\n",
        "            pass\n",
        "        else:\n",
        "            idx_crop = self.crop_pt.index(self.pt[idx])\n",
        "            idx_crop = int(idx_crop)\n",
        "            x, y, z = image.shape\n",
        "            xmin = max([0, int(self.crop_xmin[idx_crop]) - 20])\n",
        "            ymin = max([0, int(self.crop_ymin[idx_crop]) - 20])\n",
        "            zmin = max([0, int(self.crop_zmin[idx_crop]) - 20])\n",
        "            xmax = min([x, int(self.crop_xmax[idx_crop]) + 20])\n",
        "            ymax = min([y, int(self.crop_ymax[idx_crop]) + 20])\n",
        "            zmax = min([z, int(self.crop_zmax[idx_crop]) + 20])\n",
        "            image = image[xmin:xmax, ymin:ymax, zmin:zmax]\n",
        "            label = label[xmin:xmax, ymin:ymax, zmin:zmax]\n",
        "\n",
        "        sample = {'image': image, 'label': label, 'case_name': self.sample_list[idx].split('/')[-1].split('.npz')[0]}\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    args = ml_collections.ConfigDict()\n",
        "    args.max_epochs = args_.max_epochs\n",
        "    args.save_path = args_.save_path\n",
        "    args.root_path = args_.root_path\n",
        "    args.crop = args_.crop\n",
        "    args.num_classes = args_.num_classes\n",
        "    args.batch_size = args_.batch_size\n",
        "    args.seed = 42\n",
        "    args.base_lr = 0.01\n",
        "\n",
        "    # Transforms & Augmentations\n",
        "    train_transforms = transforms.Compose(\n",
        "        [RandomGenerator3d_zoom(output_size=(args.img_size, args.img_size, args.img_size))])\n",
        "    val_transforms = transforms.Compose([Reshape3d_zoom(output_size=[args.img_size, args.img_size, args.img_size])])\n",
        "\n",
        "    # Define model\n",
        "    config_net = TransUNet_configs(args.img_size)\n",
        "    model = TransUNet3d(config_net, img_size=args.img_size, num_classes=args.num_classes, zero_head=False,\n",
        "                        vis=False)\n",
        "    config = {'ds_val': fetch_dataset(base_dir=os.path.join(args.root_path, 'val'), transform=val_transforms,\n",
        "                                      crop=args_.crop),\n",
        "              'ds_train': fetch_dataset(base_dir=os.path.join(args.root_path, 'train'),\n",
        "                                        transform=train_transforms, crop=args_.crop),\n",
        "              'loss_function': DiceCELoss(include_background=False, to_onehot_y=True, softmax=True),\n",
        "              'metric': DiceMetric(include_background=False, reduction=\"mean\"),\n",
        "              'optimizer': torch.optim.Adam(model.parameters(), args.base_lr),\n",
        "              'save_interval': 50}\n",
        "\n",
        "    if args_.checkpoint == 'None':\n",
        "        pass\n",
        "    else:\n",
        "        print('loading checkpoint ', args_.checkpoint)\n",
        "        model_state = torch.load(args_.checkpoint, map_location='cpu')\n",
        "        model.load_state_dict(model_state)\n",
        "\n",
        "    trainer(args, config, model, args.save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "hN7Uh74THYNp",
        "outputId": "e651f8fd-0f24-4c3c-a388-c859fdcce79c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif __name__ == \"__main__\":\\n\\n    args = ml_collections.ConfigDict()\\n    args.max_epochs = args_.max_epochs\\n    args.save_path = args_.save_path\\n    args.root_path = args_.root_path\\n    args.crop = args_.crop\\n    args.num_classes = args_.num_classes\\n    args.batch_size = args_.batch_size\\n    args.seed = 42\\n    args.base_lr = 0.01\\n\\n    # Transforms & Augmentations\\n    train_transforms = transforms.Compose(\\n        [RandomGenerator3d_zoom(output_size=(args.img_size, args.img_size, args.img_size))])\\n    val_transforms = transforms.Compose([Reshape3d_zoom(output_size=[args.img_size, args.img_size, args.img_size])])\\n\\n    # Define model\\n    config_net = TransUNet_configs(args.img_size)\\n    model = TransUNet3d(config_net, img_size=args.img_size, num_classes=args.num_classes, zero_head=False,\\n                        vis=False)\\n    config = {\\'ds_val\\': fetch_dataset(base_dir=os.path.join(args.root_path, \\'val\\'), transform=val_transforms,\\n                                      crop=args_.crop),\\n              \\'ds_train\\': fetch_dataset(base_dir=os.path.join(args.root_path, \\'train\\'),\\n                                        transform=train_transforms, crop=args_.crop),\\n              \\'loss_function\\': DiceCELoss(include_background=False, to_onehot_y=True, softmax=True),\\n              \\'metric\\': DiceMetric(include_background=False, reduction=\"mean\"),\\n              \\'optimizer\\': torch.optim.Adam(model.parameters(), args.base_lr),\\n              \\'save_interval\\': 50}\\n\\n    if args_.checkpoint == \\'None\\':\\n        pass\\n    else:\\n        print(\\'loading checkpoint \\', args_.checkpoint)\\n        model_state = torch.load(args_.checkpoint, map_location=\\'cpu\\')\\n        model.load_state_dict(model_state)\\n\\n    trainer(args, config, model, args.save_path)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dWqEW9lfIJWd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}